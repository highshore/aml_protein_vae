# -*- coding: utf-8 -*-
"""preprocess_data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KeUkwEAB5RDp0iZnr4TXQ4GQ0TF_Qqcv
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

#Retrieve sequences from CSV
seq_df = pd.read_csv("blast_hits_raw.csv")

##Calculate average protein sequence length
seq_df["Seq_Length"] = seq_df["Full_FASTA"].apply(len)

# Calculate statistics
min_len = seq_df["Seq_Length"].min()
max_len = seq_df["Seq_Length"].max()
avg_len = seq_df["Seq_Length"].mean()

print("Minimum length:", min_len)
print("Maximum length:", max_len)
print("Average length:", avg_len)

##Fix sequence length by adding padding to both sides Padding Alphabet: "-"
'''
asdfasdfasdf-----
sadasdfasdfsdfasd
asdfadsfa--------
sdfasdfasdfadsf--
'''

# Suppose df["Full_FASTA"] contains your sequences
max_len = seq_df["Seq_Length"].max()

# Pad sequences with "-" on the right (to equal length)
#seq_df["Padded_FASTA"] = seq_df["Full_FASTA"].apply(lambda x: x.ljust(max_len, "-")) #To the right
seq_df["Padded_FASTA"] = seq_df["Full_FASTA"].apply(lambda x: x.rjust(max_len, "-")) #To the right
# Check lengths (all should be 333)
print(seq_df["Padded_FASTA"].apply(len).unique())
print(seq_df.head(10))
seq_df.to_csv("blast_hits.csv", index=False)

##Or just MSA

##Train Test Split 8:2
# For example, split 80% train, 20% test
train_df, test_df = train_test_split(seq_df, test_size=0.2, random_state=42)

print("Train size:", train_df.shape)
print("Test size:", test_df.shape)

seq_df.shape[0] ## Why only 50?

##One hot encoding: String -> Vector
##One hot encoding: String -> Vector

# Define amino acid alphabet (incl. gap '-')
aa_alphabet = "ACDEFGHIKLMNPQRSTVWY-X"
aa_to_int = {aa: i for i, aa in enumerate(aa_alphabet)}
vocab_size = len(aa_alphabet)
print("Alphabet size:", vocab_size)


def one_hot_encode_sequence(seq, max_len=333):
    # Convert each character to an integer
    int_encoded = [aa_to_int.get(aa, aa_to_int["X"]) for aa in seq]

    # One-hot encode
    one_hot = np.zeros((max_len, vocab_size), dtype=np.int8)
    for i, idx in enumerate(int_encoded):
        one_hot[i, idx] = 1
    return one_hot

# One-hot encode all sequences in training set
train_encoded = np.array([one_hot_encode_sequence(seq, max_len=333)
                          for seq in train_df["Padded_FASTA"]])

print("Shape of training data:", train_encoded.shape)
# (num_sequences, 333, vocab_size)


train_encoded_flat = train_encoded.reshape(train_encoded.shape[0], -1)
print("Flattened shape:", train_encoded_flat.shape)
# (num_sequences, 333*22)