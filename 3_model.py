# -*- coding: utf-8 -*-
"""3. model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KeUkwEAB5RDp0iZnr4TXQ4GQ0TF_Qqcv
"""

import torch.nn as nn
import torch.nn.functional as F
import torch

capacity = 64
latent_dims = 2

class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        c = capacity
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14
        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7
        self.fc_mu = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)
        self.fc_logvar = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)

    def forward(self, x, verbose=False):
        x_1 = F.relu(self.conv1(x))
        x_2 = F.relu(self.conv2(x_1))
        x_3 = x_2.view(x_2.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors
        x_mu = self.fc_mu(x_3)
        x_logvar = self.fc_logvar(x_3)

        if verbose:
          print(x_1.size(), x_2.size(), x_3.size(), x_mu.size(), x_logvar.size())

        return x_mu, x_logvar

class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        c = capacity
        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)
        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)
        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)

    def forward(self, x, verbose=False):
        x_1 = self.fc(x)
        x_2 = x_1.view(x_1.size(0), capacity*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps
        x_2 = F.relu(self.conv2(x_2))
        x_3 = torch.sigmoid(self.conv1(x_2)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss

        if verbose:
          print(x_1.size(), x_2.size(), x_3.size())

        return x_3

class VariationalAutoencoder(nn.Module):
    def __init__(self):
        super(VariationalAutoencoder, self).__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()

    def forward(self, x):
        latent_mu, latent_logvar = self.encoder(x)
        latent = self.latent_sample(latent_mu, latent_logvar)
        x_recon = self.decoder(latent)
        return x_recon, latent_mu, latent_logvar

    def latent_sample(self, mu, logvar):
        if self.training:
            # Implement the correct std and Gaussian noise for Gaussian-type decoders
            std = logvar.mul(0.5).exp_()
            eps = torch.empty_like(std).normal_()
            return eps.mul(std).add_(mu)
        else:
            return mu